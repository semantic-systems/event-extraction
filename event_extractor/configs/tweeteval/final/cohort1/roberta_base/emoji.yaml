augmenter:
  name: dropout
  num_samples: 2
data:
  batch_size: 32
  config: emoji
  gradient_accu_step: 1
  include_oos: false
  label_column: label
  name: tweet_eval
  validation: true
early_stopping:
  delta: 0
  tolerance: 5
model:
<<<<<<< HEAD:event_extractor/configs/test/scl_second_training/mlp_dropout/05/emoji.yaml
  L2_normalize_encoded_feature: false
  L2_normalize_logits: false
  contrastive:
    base_temperature: 0.3
    contrast_mode: all
    contrastive_loss_ratio: 0
    temperature: 0.5
=======
  L2_normalize_encoded_feature: true
  L2_normalize_logits: false
>>>>>>> development:event_extractor/configs/tweeteval/final/cohort1/roberta_base/emoji.yaml
  dropout_rate: 0.5
  epochs: 100
  freeze_transformer_layers: all
  from_pretrained: roberta-base
  layers:
    layer1:
      n_in: 768
      n_out: 20
  learning_rate: 1.0e-05
  load_ckpt: ./outputs/tweeteval/experiments/scl/mlp_dropout/05/emoji/seed_0/pretrained_models/emoji_best_model.pt
  num_transformer_layers: full
<<<<<<< HEAD:event_extractor/configs/test/scl_second_training/mlp_dropout/05/emoji.yaml
  output_path: ./outputs/tweeteval/experiments/scl_second_training/mlp_dropout/05/
name: emoji
seed:
- 0
=======
  output_path: ./outputs/tweeteval/experiments/cohort1/roberta_base/
name: emoji
seed:
- 0
- 1
- 2
>>>>>>> development:event_extractor/configs/tweeteval/final/cohort1/roberta_base/emoji.yaml
visualizer:
- tsne
